# Homework Week 1

> With (*) means important homework

1. Implement a network that computes XOR (arbitrary number of hidden layers); meaning: the output should be +1 for y1 y2<0 and 0 otherwise! (*)
2. Implement a network that approximately or exactly computes XOR, with just 1 hidden layer(!)
3. Visualize the results of intermediate layers in a multi-layer randomly initialized NN (meaning: take a fixed randomly initialized multi-layer network, and then throw away the layers above layer n; and directly connect layer n to the output layer; see how results change when you vary n; you can start from this [notebook](https://github.com/yohanesnuwara/ML_for_physicist/blob/master/Week%201/01_MachineLearning_Basics_NeuralNetworksPython.ipynb)) (*)
4. What happens when you change the spread of the random weights? Smart weight initialization is an important point for NN training.
5. Explore cases of curve fitting where there are several (non-equivalent) local minima. Is sampling noise helpful (i.e. the noise that comes about because of the small number of x samples)?
